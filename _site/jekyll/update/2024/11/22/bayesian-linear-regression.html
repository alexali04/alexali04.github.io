  <!DOCTYPE html>
  <html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>An Anthology for Bayesian Linear Regresion | Alex’s Blog</title>
<meta name="generator" content="Jekyll v4.3.4" />
<meta property="og:title" content="An Anthology for Bayesian Linear Regresion" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="1. Introduction Things are nice when they’re simple. As far as curves go, linear is about as nice as it gets." />
<meta property="og:description" content="1. Introduction Things are nice when they’re simple. As far as curves go, linear is about as nice as it gets." />
<link rel="canonical" href="http://localhost:4000/jekyll/update/2024/11/22/bayesian-linear-regression.html" />
<meta property="og:url" content="http://localhost:4000/jekyll/update/2024/11/22/bayesian-linear-regression.html" />
<meta property="og:site_name" content="Alex’s Blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2024-11-22T00:07:44-05:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="An Anthology for Bayesian Linear Regresion" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2024-11-22T00:07:44-05:00","datePublished":"2024-11-22T00:07:44-05:00","description":"1. Introduction Things are nice when they’re simple. As far as curves go, linear is about as nice as it gets.","headline":"An Anthology for Bayesian Linear Regresion","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/jekyll/update/2024/11/22/bayesian-linear-regression.html"},"url":"http://localhost:4000/jekyll/update/2024/11/22/bayesian-linear-regression.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Alex&apos;s Blog" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Alex&#39;s Blog</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/2024-11-22-welcome-to-jekyll.txt">Welcome to Jekyll!</a><a class="page-link" href="/about/">About</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
        <div class="wrapper">
          <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">An Anthology for Bayesian Linear Regresion</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2024-11-22T00:07:44-05:00" itemprop="datePublished">Nov 22, 2024
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <h1 id="1-introduction">1. <strong>Introduction</strong></h1>
<hr />
<p>Things are nice when they’re simple. As far as curves go, linear is about as nice as it gets.</p>

<p>I present to you a rough anthology of linear regression. By this, I mean several different approaches to linear regression. There are several computational tricks / alternative derivations</p>

<h1 id="2-problem-set-up">2. <strong>Problem Set up</strong></h1>
<hr />

<p>We begin with a dataset $\mathcal{D} = {(x_i, y_i)}_{i= 1}^n$. Let $y \in \mathbb{R}, x \in \mathbb{R}^d$. We want to find a linear relationship between features $x_i$ and noisy target values $y_i$, i.e.:</p>

\[y_i = f(x_i) + \epsilon_x\]

<p>If we believe the underlying relationship between $y$ and $x$ is in fact linear, we set</p>

\[f(x_i) = w^T x_i\]

<p>We can collect the target values $y_1, \dots, y_n$ into a vector and the feature vectors $x_1, \dots, x_N$ into a <strong>design matrix</strong> $X$ where the row $X_i = x_i$. This ensures each $y_i = x_i^T w$.</p>

\[y = X w + \epsilon_x\]

<p>Note that this represents $y$ as a column vector.</p>

<h1 id="3-minimize-mse">3. <strong>Minimize MSE</strong></h1>
<hr />

<p>The residual (error between $Xw$ and $y$) is defined as:</p>

\[e = y - Xw\]

<p>We want to find the $\hat{w}$ that minimizes the <strong>mean-squared error</strong>:</p>

\[\hat{w} = \argmin_{w} e^T e = \argmin_{w} \ (y - Xw)^T (y - Xw)\]

<p>For optimization problems like this, we can just take the first derivative w.r.t $w$ and set to $0$.</p>

\[0 = \frac{\partial e^T e}{\partial w} = \frac{\partial}{\partial w} (y - Xw)^T (y - Xw)\]

\[= \frac{\partial}{\partial w} (y^T y + w^T X^T X w - 2 w^T X^T y)\]

<p>There are two ways to treat matrix derivatives. Numerator layout (treating the gradient as row vector) and denominator layout (treating the gradient as a column vector). Both will work.</p>

<p>Here are the rules we will use (treating the gradient as a row vector). To treat the gradient as a column vector, take the transpose.</p>

\[\frac{\partial}{\partial x} x^T a = \frac{\partial}{\partial x} a^T x = a^T\]

\[\frac{\partial}{\partial x} x^T A x = x^T (A + A^T)\]

<h2 id="31-gradient-is-a-row-vector">3.1 Gradient is a Row Vector</h2>

<p>The first term ($y^T y$) is not dependent on $w$ so it goes to zero. The second term becomes $2 w^T X^T X$. The third term becomes $2 y^T X$. So we have:</p>

\[0 = 2 w^T X^T X - 2 y^T X\]

\[y^T X = w^T X^T X\]

\[w^T = y^T X (X^T X)^{-1}\]

\[w = (X^T X)^{-1} X^T y\]

<p>If a symmetric matrix is non-singular (read: invertible), it’s inverse is also symmetric.</p>

<h2 id="32-gradient-is-a-column-vector">3.2 Gradient is a Column Vector</h2>

<p>The second term becomes $2 X^T X w$ and the third becomes $2 X^T y$.</p>

\[0 = 2 X^T X w - 2 X^T y\]

<p>Skipping ahead…</p>

\[w = (X^T X)^{-1} X^T y\]

<p>Okay, this was all fairly simple. We can also read this as $w = \frac{\text{Cov}(X, y)}{\text{Var}(X)}$ to view this from another angle.</p>

<p>I hope everything seems relatively well-motivated thus far. However, there are certain assumptions that have already been made as well as other issues that have been glossed over.</p>

<p>Prominently, we defined the loss function to be the mean-squared error $\text{MSE}(y, Xw)$. This is equivalent to minimizing the $L_2$ norm of the residuals.</p>

<p>But why mean-squared error? Why not mean absolute error (MAE) or mean-cubic error?</p>

<p>There are many reasons why MSE is a empirically nice loss function to use. It’s differentiable unlike MAE and doesn’t place as much weight on outliers as mean errors of higher order terms does. Mathematically $L_2$ is also a Hilbert space while $L_p$ for $p \geq 1, p \neq 2$ is not. But we still <em>chose</em> MSE.</p>

<p>I find it everything fits much nicer in my head when we don’t need to choose an arbitrary loss function but are forced into it by the assumptions we make. We’ll now do linear regression probabilistically.</p>

<h1 id="4-maximum-likelihood-estimation">4. <strong>Maximum Likelihood Estimation</strong></h1>
<hr />

<p>Let’s begin by placing a distribution over the noise. Say, for example, we choose $\epsilon_x \sim \mathcal{N}(0, \sigma^2)$. So we have:</p>

\[\epsilon_x \sim \mathcal{N}(0, \sigma^2)\]

\[y_i \sim \mathcal{N}(w^T x_i, \sigma^2)\]

<p>The <strong>likelihood function</strong> is:</p>

\[\mathcal{L}(w \mid \mathcal{D}) = p(\mathcal{D} \mid w)\]

<p>We want to maximize the likelihood of observing the data. Our data points $y_i$ are i.i.d. since they’re all drawn from $\mathcal{N}(w^T x_i, \sigma^2)$ (identical) and are conditionally independent on the mean and variance. So we have:</p>

\[p(\mathcal{D} \mid w) = p(y_1, \dots, y_N \mid w, x_1, \dots, x_n, \sigma^2)\]

\[= \prod_{i = 1}^n \mathcal{N}(y_i ; w^T x_i, \sigma^2)\]

\[= \prod_{i = 1}^n \frac{1}{\sqrt{2 \pi \sigma^2}} \exp \left \{ - \frac{1}{2 \sigma^2} (y_i - w^T x_i)^2    \right \}\]

<p>The product of exponentials becomes a sum within the exponential. So we have:</p>

\[= \frac{1}{(2 \pi \sigma^2)^\frac{n}{2}} \exp \left \{ - \frac{1}{2 \sigma^2} \sum_{i = 1}^n (y_i - w^T x_i)^2    \right \}\]

<p>It’s nice to re-write these as inner products.</p>

\[= \frac{1}{(2 \pi \sigma^2)^\frac{n}{2}} \exp \left \{ - \frac{1}{2 \sigma^2} (y - Xw)^T (y - Xw)    \right \}\]

<p>Now, we want to maximize this likelihood with respect to $w$. Since the logarithm is a monotonic transformation, the log-likelihood and the likelihood have the same extrema. So, we have:</p>

\[w_{\text{MLE}} = \argmax_w \ p(\mathcal{D} \mid w) = \argmax_w \ \log p(\mathcal{D} \mid w)\]

<p>So we have:</p>

\[\log p(\mathcal{D} \mid w) = - \frac{1}{2 \sigma^2} (y^Ty + w^T X^T X w - 2 y^T X w) - \frac{n}{2} \log(2 \pi \sigma^2)\]

\[0 = \frac{\partial}{\partial w} \log p(D \mid w) = - \frac{1}{2 \sigma^2} \frac{\partial}{\partial w} (y^T y + w^T X^T X w - 2 y^T X w)\]

<p>For convenience’s sake, I’ll treat the gradient as a column vector although I generally prefer the other way.</p>

<p>So we have:</p>

\[0 = 2 X^T X w - 2 X^T y\]

\[X^T y = X^T X w\]

\[w_{\text{MLE}} = (X^T X)^{-1} X^T y\]

<p>This is no different from what we’ve already seen. But we can also derive a maximum likelihood estimate for the noise variance.</p>

\[0 = \frac{\partial}{\partial \sigma^2} \log p(\mathcal{D} \mid w) = \frac{\partial}{\partial \sigma^2} \left [ - \frac{1}{2 \sigma^2} (y^Ty + w^T X^T X w - 2 y^T X w) - \frac{n}{2} \log(2 \pi \sigma^2) \right ]\]

\[0 = - \frac{2 n \pi}{4 \pi \sigma^2} + \frac{1}{2 \sigma^4} (y - Xw)^T (y - Xw)\]

\[\frac{n}{\sigma^2} = \frac{1}{\sigma^4} (y - Xw)^T (y - Xw)\]

\[\sigma^2_{\text{MLE}} = \frac{1}{n} (y - Xw)^T (y - Xw)\]

<p>This is a little better - its nice to be able to estimate the variance of the noise as well.</p>

<h2 id="41-issues-with-mle">4.1 <strong>Issues with MLE</strong></h2>

<p>There are several issues with maximum likelihood estimation. But the most glaring one, to me at least, is that it fundamentally answers the wrong question. We don’t really care about the probability of observing the data given some parameter setting. We care about the probability of some parameter setting given the data.</p>

<h1 id="50-map">5.0 MAP</h1>

<p>One way to do this is with <strong>MAP</strong> or <strong>Maximum a Posteriori</strong> estimation.</p>

<p>Bayes Rule states:</p>

\[p(w \mid \mathcal{D}) = \frac{p(\mathcal{D} \mid w) p(w)}{p(\mathcal{D})}\]

<p>The left-hand side is called the <strong>posterior distribution</strong>. It’s proportional to the likelihood multiplied with the prior distribution. The prior represents our beliefs about the data before we have observed the data - the posterior represents our updated beliefs after having observed the data (likelihood).</p>

<p>So we need to place a prior distribution on $w$. So we can specify:</p>

\[w \sim \mathcal{N}(0, \alpha^2 I)\]

\[\epsilon_x \sim \mathcal{N}(0, \sigma^2)\]

<p>So the joint distribution can be written as a multivariate Gaussian.</p>

\[y \mid w, X, \sigma^2 \sim \mathcal{N}(Xw, \sigma^2 I)\]

<p>The MAP estimate $w_{\text{MAP}}$ is the parameter setting which maximizes the probability of the parameter given the data. So we want:</p>

\[w_{\text{MAP}} = \argmax_{w} p(w \mid \mathcal{D})\]

<p>Again, we can take the logarithm, so we have:</p>

\[w_{\text{MAP}} = \argmax_{w} \ \log p(w \mid \mathcal{D})\]

\[= \argmax_{w} \ \log p(\mathcal{D} \mid w) + \log p(w) - \log(\mathcal{D})\]

<p>The denominator of Bayes rule is called the marginal likelihood or the partition function or the evidence. Since it doesn’t depend on $w$, we can just ignore it in finding $w_{\text{MAP}}$. So we have:</p>

\[0 = \frac{\partial}{\partial w} \log p(\mathcal{D} \mid w) + \log p(w)\]

<p>Taking the logarithm of normal distributions,</p>

\[= \frac{\partial}{\partial w} \left [ 
- \frac{1}{2 \sigma^2} (y - Xw)^T (y - Xw) - \frac{n}{2} \log (2 \pi \sigma^2) - \frac{1}{2 \alpha^2} w^T w - \frac{d}{2} \log (2 \pi \alpha^2)
\right ]\]

<p>We’ve performed bits and pieces of this derivative above. So we get</p>

\[0 = \frac{X^T y}{\sigma^2} - \frac{w}{\alpha^2} - \frac{X^T X w}{\sigma^2}\]

\[0 = X^T y - \frac{\sigma^2 w}{\alpha^2} - X^T X w\]

<p>Define $\lambda = \frac{\sigma^2}{\alpha^2}$. Then, we have:</p>

\[0 = X^T y - \lambda w - X^T X w\]

\[(X^T X + \lambda I)w = X^T y\]

\[w_{\text{MAP}} = (X^T X + \lambda I)^{-1} X^T y\]

<p>We can reach this formula by finding:</p>

\[w_{\text{MAP}} = \argmin_{w} (y - Xw)^T (y - Xw) + \lambda w^T w\]

<p>In other words, we can recover $L_2$ regularization by assuming Gaussian noise and a Gaussian prior. I like this probabilistic approach much better because it all feels very motivated from our assumptions.</p>

<p>We can also derive MAP solutions for the noise and weight variances. First, the MAP solution for the noise variance is the same as the MLE solution because it’s not affected by the prior.</p>

\[0 = \frac{\partial}{\partial \alpha^2} \left [ 
 - \frac{1}{2 \alpha^2} w^T w - \frac{d}{2} \log (2 \pi \alpha^2)
\right ]\]

\[= \frac{1}{2 \alpha^4} w^T w - \frac{d}{2 \alpha^2}\]

\[\alpha^2_{\text{MAP}} = \frac{1}{d} w_{\text{MAP}}^T w_{\text{MAP}}\]

<p>If we send the prior variance of the weight to $0$, i.e. $\alpha^2 \to 0$, then the regularization coefficient $\lambda$ grows very large so the weights move towards $0$. Similarly, if we send the noise variance $ \sigma^2 \to \infty$, we’ll see a similar result.</p>


  </div><a class="u-url" href="/jekyll/update/2024/11/22/bayesian-linear-regression.html" hidden></a>
</article>

        </div>
      </main>

      <script>
        MathJax = {
          tex: {
            inlineMath: [['$', '$'], ['\\(', '\\)']],
            displayMath: [['$$', '$$'], ['\\[', '\\]']],
            processEscapes: true,
            tags: "ams",
            packages: {'[+]': ['ams']},
            macros: {
              argmax: "\\mathop{\\rm arg\\,max}",
              argmin: "\\mathop{\\rm arg\\,min}"
            }
          },
          options: {
            skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
          }
        };
      </script>
      <!-- Load MathJax -->
      <script type="text/javascript" async
              src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
      </script><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Alex&#39;s Blog</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Alex&#39;s Blog</li><li><a class="u-email" href="mailto:alexali000@gmail.com">alexali000@gmail.com</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/alexali04"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">alexali04</span></a></li><li><a href="https://www.twitter.com/N%2FA"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">N/A</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>This is Alex&#39;s blog for machine learning.</p>
      </div>
    </div>

  </div>

</footer>
</body>

  </html>
