<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2024-11-22T20:13:04-05:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Alex’s Blog</title><subtitle>This is Alex&apos;s blog for machine learning.</subtitle><entry><title type="html">An Anthology for Bayesian Linear Regresion</title><link href="http://localhost:4000/jekyll/update/2024/11/22/bayesian-linear-regression.html" rel="alternate" type="text/html" title="An Anthology for Bayesian Linear Regresion" /><published>2024-11-22T00:07:44-05:00</published><updated>2024-11-22T00:07:44-05:00</updated><id>http://localhost:4000/jekyll/update/2024/11/22/bayesian-linear-regression</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2024/11/22/bayesian-linear-regression.html"><![CDATA[<h1 id="1-introduction">1. <strong>Introduction</strong></h1>
<hr />
<p>Things are easy when they’re simple. As far as curves go, linear curves are about as nice as it gets.</p>

<p>I present to you a rough anthology of linear regression. By this, I mean several different approaches to linear regression. There are several computational tricks / alternative derivations which are shorter and easier. I’ll go through some of them at the end of the article but it’s probably valuable to do it the painful way at least once.</p>

<h1 id="2-problem-set-up">2. <strong>Problem Set up</strong></h1>
<hr />

<p>We begin with a dataset $\mathcal{D} = {(x_i, y_i)}_{i= 1}^n$. Let $y \in \mathbb{R}, x \in \mathbb{R}^d$. We want to find a linear relationship between features $x_i$ and noisy target values $y_i$, i.e.:</p>

\[y_i = f(x_i) + \epsilon_x\]

<p>If we believe the underlying relationship between $y$ and $x$ is in fact linear, we set</p>

\[f(x_i) = w^T x_i\]

<p>where $w$ represents the weights or scaling coefficients.</p>

<p>We can collect the target values $y_1, \dots, y_n$ into a vector and the feature vectors $x_1, \dots, x_N$ into a <strong>design matrix</strong> $X$ where the row $X_i = x_i$. This ensures each $y_i = x_i^T w$.</p>

\[y = X w + \epsilon_x\]

<p>Note that this represents $y$ as a column vector.</p>

<h1 id="3-minimize-mse">3. <strong>Minimize MSE</strong></h1>
<hr />

<p>The residual (error between $Xw$ and $y$) is defined as:</p>

\[e = y - Xw\]

<p>We want to find the $\hat{w}$ that minimizes the <strong>mean-squared error</strong>:</p>

\[\hat{w} = \argmin_{w} e^T e = \argmin_{w} \ (y - Xw)^T (y - Xw)\]

<p>For optimization problems like this, we can just take the first derivative w.r.t $w$ and set to $0$.</p>

\[0 = \frac{\partial e^T e}{\partial w} = \frac{\partial}{\partial w} (y - Xw)^T (y - Xw)\]

\[= \frac{\partial}{\partial w} (y^T y + w^T X^T X w - 2 w^T X^T y)\]

<p>There are two ways to treat matrix derivatives. Numerator layout (treating the gradient as row vector) and denominator layout (treating the gradient as a column vector). This actually caused me great grief when I was learning this on my own but I’ve kinda gotten over it. So both views are presented here.</p>

<p>Here are the rules we will use (treating the gradient as a row vector). To treat the gradient as a column vector, take the transpose.</p>

\[\frac{\partial}{\partial x} x^T a = \frac{\partial}{\partial x} a^T x = a^T\]

\[\frac{\partial}{\partial x} x^T A x = x^T (A + A^T)\]

<p>To verify these rules for yourself, you can just convert these matrix-vector products into sums.</p>

<h4 id="31-gradient-is-a-row-vector">3.1 <strong>Gradient is a Row Vector</strong></h4>
<hr />

<p>The first term ($y^T y$) is not dependent on $w$ so it goes to zero. The second term becomes $2 w^T X^T X$. The third term becomes $2 y^T X$. So we have:</p>

\[0 = 2 w^T X^T X - 2 y^T X\]

\[y^T X = w^T X^T X\]

\[w^T = y^T X (X^T X)^{-1}\]

\[w = (X^T X)^{-1} X^T y\]

<p>If a symmetric matrix is non-singular, it’s inverse is also symmetric.</p>

<h4 id="32-gradient-is-a-column-vector">3.2 <strong>Gradient is a Column Vector</strong></h4>
<hr />

<p>The second term becomes $2 X^T X w$ and the third becomes $2 X^T y$.</p>

\[0 = 2 X^T X w - 2 X^T y\]

<p>Skipping ahead…</p>

\[w = (X^T X)^{-1} X^T y\]

<p>Okay, this was all fairly simple. We can also read this as $w = \frac{\text{Cov}(X, y)}{\text{Var}(X)}$ to view this from another angle.</p>

<p>I hope everything seems relatively well-motivated thus far. However, there are certain assumptions that have already been made as well as other issues that have been glossed over.</p>

<p>Prominently, we defined the loss function to be the mean-squared error $\text{MSE}(y, Xw)$. This is equivalent to minimizing the $L_2$ norm of the residuals.</p>

<p>But why mean-squared error? Why not mean absolute error (MAE) or mean-cubic error?</p>

<p>There are many reasons why MSE is a empirically nice loss function to use. It’s differentiable unlike MAE and doesn’t place as much weight on outliers as mean errors of higher order terms does. Mathematically $L_2$ is also a Hilbert space while $L_p$ for $p \geq 1, p \neq 2$ is not. But we still <em>chose</em> MSE.</p>

<p>I find it everything fits much nicer in my head when we don’t need to choose an arbitrary loss function but are forced into it by the assumptions we make. We’ll now do linear regression probabilistically.</p>

<h1 id="4-maximum-likelihood-estimation">4. <strong>Maximum Likelihood Estimation</strong></h1>
<hr />

<p>Let’s begin by placing a distribution over the noise. Say, for example, we choose $\epsilon_x \sim \mathcal{N}(0, \sigma^2)$. So we have:</p>

\[\epsilon_x \sim \mathcal{N}(0, \sigma^2)\]

\[y_i \sim \mathcal{N}(w^T x_i, \sigma^2)\]

<p>The <strong>likelihood function</strong> is:</p>

\[\mathcal{L}(w \mid \mathcal{D}) = p(\mathcal{D} \mid w)\]

<p>We want to maximize the likelihood of observing the data (what does this mean?). Our data points $y_i$ are i.i.d. since they’re all drawn from $\mathcal{N}(w^T x_i, \sigma^2)$ (identical) and are conditionally independent on the mean and variance. So we have:</p>

\[p(\mathcal{D} \mid w) = p(y_1, \dots, y_N \mid w, x_1, \dots, x_n, \sigma^2)\]

\[= \prod_{i = 1}^n \mathcal{N}(y_i ; w^T x_i, \sigma^2)\]

\[= \prod_{i = 1}^n \frac{1}{\sqrt{2 \pi \sigma^2}} \exp \left \{ - \frac{1}{2 \sigma^2} (y_i - w^T x_i)^2    \right \}\]

<p>The product of exponentials becomes a sum within the exponential. So we have:</p>

\[= \frac{1}{(2 \pi \sigma^2)^\frac{n}{2}} \exp \left \{ - \frac{1}{2 \sigma^2} \sum_{i = 1}^n (y_i - w^T x_i)^2    \right \}\]

<p>It’s nice to re-write these as inner products.</p>

\[= \frac{1}{(2 \pi \sigma^2)^\frac{n}{2}} \exp \left \{ - \frac{1}{2 \sigma^2} (y - Xw)^T (y - Xw)    \right \}\]

<p>Now, we want to maximize this likelihood with respect to $w$. Since the logarithm is a monotonic transformation, the log-likelihood and the likelihood have the same extrema. So, we have:</p>

\[w_{\text{MLE}} = \argmax_w \ p(\mathcal{D} \mid w) = \argmax_w \ \log p(\mathcal{D} \mid w)\]

<p>So we have:</p>

\[\log p(\mathcal{D} \mid w) = - \frac{1}{2 \sigma^2} (y^Ty + w^T X^T X w - 2 y^T X w) - \frac{n}{2} \log(2 \pi \sigma^2)\]

\[0 = \frac{\partial}{\partial w} \log p(D \mid w) = - \frac{1}{2 \sigma^2} \frac{\partial}{\partial w} (y^T y + w^T X^T X w - 2 y^T X w)\]

<p>For convenience’s sake, I’ll treat the gradient as a column vector although I generally prefer the other way.</p>

<p>So we have:</p>

\[0 = 2 X^T X w - 2 X^T y\]

\[X^T y = X^T X w\]

\[w_{\text{MLE}} = (X^T X)^{-1} X^T y\]

<p>This is no different from what we’ve already seen. But we can also derive a maximum likelihood estimate for the noise variance.</p>

\[0 = \frac{\partial}{\partial \sigma^2} \log p(\mathcal{D} \mid w) = \frac{\partial}{\partial \sigma^2} \left [ - \frac{1}{2 \sigma^2} (y^Ty + w^T X^T X w - 2 y^T X w) - \frac{n}{2} \log(2 \pi \sigma^2) \right ]\]

\[0 = - \frac{2 n \pi}{4 \pi \sigma^2} + \frac{1}{2 \sigma^4} (y - Xw)^T (y - Xw)\]

\[\frac{n}{\sigma^2} = \frac{1}{\sigma^4} (y - Xw)^T (y - Xw)\]

\[\sigma^2_{\text{MLE}} = \frac{1}{n} (y - Xw)^T (y - Xw)\]

<p>This is a little better - its nice to be able to estimate the variance of the noise as well.</p>

<h4 id="41-issues-with-mle">4.1 <strong>Issues with MLE</strong></h4>
<hr />

<p>There are several issues with maximum likelihood estimation. But the most glaring one, to me at least, is that it fundamentally answers the wrong question. We don’t really care about the probability of observing the data given some parameter setting. We care about the probability of some parameter setting given the data.</p>

<h1 id="5-map">5. <strong>MAP</strong></h1>
<hr />

<p>One way to do this is with <strong>MAP</strong> or <strong>Maximum a Posteriori</strong> estimation.</p>

<p>Bayes Rule states:</p>

\[p(w \mid \mathcal{D}) = \frac{p(\mathcal{D} \mid w) p(w)}{p(\mathcal{D})}\]

<p>The left-hand side is called the <strong>posterior distribution</strong>. It’s proportional to the likelihood multiplied with the prior distribution. The prior represents our beliefs about the data before we have observed the data - the posterior represents our updated beliefs after having observed the data (likelihood).</p>

<p>So we need to place a prior distribution on $w$. So we can specify:</p>

\[w \sim \mathcal{N}(0, \alpha^2 I)\]

\[\epsilon_x \sim \mathcal{N}(0, \sigma^2)\]

<p>So the joint distribution can be written as a multivariate Gaussian.</p>

\[y \mid w, X, \sigma^2 \sim \mathcal{N}(Xw, \sigma^2 I)\]

<p>The MAP estimate $w_{\text{MAP}}$ is the parameter setting which maximizes the probability of the parameter given the data. So we want:</p>

\[w_{\text{MAP}} = \argmax_{w} p(w \mid \mathcal{D})\]

<p>Again, we can take the logarithm, so we have:</p>

\[w_{\text{MAP}} = \argmax_{w} \ \log p(w \mid \mathcal{D})\]

\[= \argmax_{w} \ \log p(\mathcal{D} \mid w) + \log p(w) - \log(\mathcal{D})\]

<p>The denominator of Bayes rule is called the marginal likelihood or the partition function or the evidence. Since it doesn’t depend on $w$, we can just ignore it in finding $w_{\text{MAP}}$. So we have:</p>

\[0 = \frac{\partial}{\partial w} \log p(\mathcal{D} \mid w) + \log p(w)\]

<p>Taking the logarithm of normal distributions,</p>

\[= \frac{\partial}{\partial w} \left [ 
- \frac{1}{2 \sigma^2} (y - Xw)^T (y - Xw) - \frac{n}{2} \log (2 \pi \sigma^2) - \frac{1}{2 \alpha^2} w^T w - \frac{d}{2} \log (2 \pi \alpha^2)
\right ]\]

<p>We’ve performed bits and pieces of this derivative above. So we get</p>

\[0 = \frac{X^T y}{\sigma^2} - \frac{w}{\alpha^2} - \frac{X^T X w}{\sigma^2}\]

\[0 = X^T y - \frac{\sigma^2 w}{\alpha^2} - X^T X w\]

<p>Define $\lambda = \frac{\sigma^2}{\alpha^2}$. Then, we have:</p>

\[0 = X^T y - \lambda w - X^T X w\]

\[(X^T X + \lambda I)w = X^T y\]

\[w_{\text{MAP}} = (X^T X + \lambda I)^{-1} X^T y\]

<p>We can reach this formula by finding:</p>

\[w_{\text{MAP}} = \argmin_{w} (y - Xw)^T (y - Xw) + \lambda w^T w\]

<p>In other words, we can recover $L_2$ regularization by assuming Gaussian noise and a Gaussian prior. I like this probabilistic approach much better because it all feels very motivated from our assumptions.</p>

<p>We can also derive MAP solutions for the noise and weight variances. First, the MAP solution for the noise variance is the same as the MLE solution because it’s not affected by the prior.</p>

\[0 = \frac{\partial}{\partial \alpha^2} \left [ 
 - \frac{1}{2 \alpha^2} w^T w - \frac{d}{2} \log (2 \pi \alpha^2)
\right ]\]

\[= \frac{1}{2 \alpha^4} w^T w - \frac{d}{2 \alpha^2}\]

\[\alpha^2_{\text{MAP}} = \frac{1}{d} w_{\text{MAP}}^T w_{\text{MAP}}\]

<p>If we send the prior variance of the weight to $0$, i.e. $\alpha^2 \to 0$, then the regularization coefficient $\lambda$ grows very large so the weights move towards $0$. Similarly, if we send the noise variance $ \sigma^2 \to \infty$, we’ll see a similar result.</p>

<h1 id="6-bayesian-linear-regression">6. <strong>Bayesian Linear Regression</strong></h1>

<h4 id="61-marginal-likelihood">6.1 <strong>Marginal Likelihood</strong></h4>

<h4 id="62-posterior-distribution">6.2 <strong>Posterior Distribution</strong></h4>

<h1 id="7-additional-notes">7. Additional Notes</h1>

<p>I was initially confused by the notion of likelihood. What does the likelihood term $p(\mathcal{D} \mid w)$, the “probability of observing the data” even mean?</p>

<p>We assume that the data was generated as part of a causal process paramaterized by $w$. The likelihood of the data is the probability that the parameter setting $w$ generated the data.</p>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[1. Introduction Things are easy when they’re simple. As far as curves go, linear curves are about as nice as it gets.]]></summary></entry></feed>